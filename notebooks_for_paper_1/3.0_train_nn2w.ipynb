{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NN to generate total latent heat given only forcing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "from IPython.display import SVG\n",
    "from tqdm.keras import TqdmCallback\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import plot_model, model_to_dot\n",
    "from tensorflow.keras import layers\n",
    "import dask.dataframe as dd\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "\n",
    "sns.set_context('talk')\n",
    "mpl.style.use('seaborn-bright')\n",
    "mpl.rcParams['figure.figsize'] = (18, 12)\n",
    "\n",
    "#strategy = tf.distribute.MirroredStrategy()\n",
    "#print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "dtype='float32'\n",
    "K.set_floatx(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = os.listdir('../sites')\n",
    "bad_sites = []\n",
    "sim_sites = [s for s in sites if s not in bad_sites]\n",
    "\n",
    "seed = 50334\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(sim_sites)\n",
    "len(sim_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_dict = {s: xr.open_dataset(f'../sites/{s}/forcings/{s}.nc').isel(hru=0, drop=True).load() \n",
    "             for s in sim_sites}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_attr = {s: xr.open_dataset(f'../sites/{s}/params/local_attributes.nc').isel(hru=0, drop=True).load() \n",
    "             for s in sim_sites}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_outp = {s: xr.open_dataset(f'../sites/{s}/output/template_output_{s}_timestep.nc').isel(hru=0, drop=True).load() \n",
    "             for s in sim_sites}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_parm = {s: xr.open_dataset(f'../sites/{s}/params/parameter_trial.nc').isel(hru=0, drop=True).load() \n",
    "             for s in sim_sites}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_time(sim, obs, roundto='min'):\n",
    "    sim['time'] = sim['time'].dt.round(roundto)\n",
    "    obs['time'] = obs['time'].dt.round(roundto)\n",
    "    sim_start = sim['time'].values[1]\n",
    "    sim_stop = sim['time'].values[-2]\n",
    "    obs_start = obs['time'].values[1]\n",
    "    obs_stop = obs['time'].values[-2]\n",
    "    start = max(sim_start, obs_start)\n",
    "    stop = min(sim_stop, obs_stop)\n",
    "    return slice(start, stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sim_sites:\n",
    "    # trim times to match\n",
    "    in_ds = site_dict[s]\n",
    "    outp_ds = site_outp[s]\n",
    "    ts = trim_time(in_ds, outp_ds)\n",
    "    in_ds = in_ds.sel(time=ts)\n",
    "    outp_ds = outp_ds.sel(time=ts)\n",
    "    site_dict[s] = in_ds\n",
    "    site_outp[s] = outp_ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfold = 4\n",
    "kfold_test_sites = np.array(sim_sites).reshape(nfold, -1)\n",
    "\n",
    "kfold_train_sites = np.vstack([\n",
    "    list(set(sim_sites) - set(test_sites)) for test_sites in kfold_test_sites\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_train_sites.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysumma.plotting as psp\n",
    "import pysumma.utils as psu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_single_site(in_ds, attr_ds, outp_ds, parm_ds, use_mask=True):\n",
    "   \n",
    "    #---------------------------------------------------------------------------\n",
    "    # Forcings\n",
    "    #---------------------------------------------------------------------------\n",
    "    airtemp   = (((in_ds['airtemp'].values / 27.315) - 10) / 2) + 0.5\n",
    "    spechum   = (in_ds['spechum'].values * 50)  \n",
    "    swradatm  = np.cbrt((in_ds['SWRadAtm'].values / 1000) )\n",
    "    lwradatm  = in_ds['LWRadAtm'].values / (2 * 273.16)\n",
    "    pptrate   = 10 * np.cbrt(in_ds['pptrate'].values)\n",
    "    airpres   = (10 - (in_ds['airpres'].values / 10132.5)) / 2\n",
    "    windspd   = in_ds['windspd'].values / 10\n",
    "    mask      = in_ds['gap_filled'].values\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # Parameters\n",
    "    #---------------------------------------------------------------------------\n",
    "    soiltype = attr_ds['soilTypeIndex'].values[()] * np.ones_like(mask) / 12\n",
    "    vegtype = attr_ds['vegTypeIndex'].values[()] * np.ones_like(mask) / 12\n",
    "\n",
    "    canheight = parm_ds['heightCanopyTop'].values[()] * np.ones_like(mask) / 30\n",
    "    vcmax = parm_ds['vcmax_Kn'].values[()] * np.ones_like(mask)\n",
    "    canopyWettingFactor = parm_ds['canopyWettingFactor'].values[()] * np.ones_like(mask)\n",
    "    thetasat = parm_ds['theta_sat'].values[()] * np.ones_like(mask)\n",
    "    thetares = parm_ds['theta_res'].values[()] * np.ones_like(mask)\n",
    "    laiscale = parm_ds['laiScaleParam'].values[()] * np.ones_like(mask) / 3\n",
    "    rootdepth = parm_ds['rootingDepth'].values[()] * np.ones_like(mask) / 5\n",
    "    \n",
    "    vp_air = outp_ds['scalarVPair'].values / 2000\n",
    "    lai = outp_ds['scalarLAI'].values / 12\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # Soil moistures\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Surface moisture\n",
    "    surf_idx = -len(outp_ds.midSoil)\n",
    "    surf_sm = outp_ds['mLayerVolFracWat'].copy(deep=True)\n",
    "    vmask = surf_sm != -9999\n",
    "    surf_sm.values = psp.utils.justify(surf_sm.where(vmask).values)\n",
    "    surf_sm = surf_sm.isel(midToto=surf_idx).values\n",
    "    \n",
    "    # Transpirable water\n",
    "    depth = outp_ds['mLayerHeight'].copy(deep=True)\n",
    "    dmask = depth != -9999\n",
    "    depth.values = psp.utils.justify(depth.where(dmask).values)\n",
    "    depth = depth.isel(midToto=slice(surf_idx, None))   \n",
    "    transpirable = outp_ds['mLayerVolFracWat'].copy(deep=True)\n",
    "    vmask = transpirable != -9999\n",
    "    transpirable.values = psp.utils.justify(transpirable.where(vmask).values)\n",
    "    transpirable = (outp_ds['mLayerRootDensity'] * (\n",
    "                    transpirable.sel(midToto=slice(surf_idx, None)).values )).sum(dim='midSoil')\n",
    "    transpirable = transpirable.where(transpirable > 0, other=0).values\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # Soil Temperatures and snow state\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Surface temperature\n",
    "    var = outp_ds['mLayerTemp'].copy()\n",
    "    vmask = var != -9999\n",
    "    var.values = psp.utils.justify(var.where(vmask).values)\n",
    "    surf_temp = var.isel(midToto=surf_idx) \n",
    "    surf_temp   = (((surf_temp.values / 27.315) - 10) / 2) + 0.5\n",
    "\n",
    "    # Depth averaged temperature\n",
    "    avg_temp = var.isel(midToto=slice(surf_idx, None)).sum(dim='midToto') / (-surf_idx)\n",
    "    avg_temp = (depth * avg_temp).sum(dim='midToto') / depth.sum(dim='midToto')\n",
    "    avg_temp   = (((avg_temp.values / 27.315) - 10) / 2) + 0.5\n",
    "\n",
    "    # Snow presence\n",
    "    swe = np.minimum(outp_ds['scalarSWE'].copy().values, 1) \n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # Arrange inputs and outputs\n",
    "    #---------------------------------------------------------------------------    \n",
    "    train_input = np.vstack([airtemp, \n",
    "                             spechum, \n",
    "                             swradatm, \n",
    "                             lwradatm, \n",
    "                             pptrate, \n",
    "                             airpres, \n",
    "                             windspd, \n",
    "                             vegtype, \n",
    "                             soiltype,\n",
    "                             canheight, \n",
    "                             vcmax, \n",
    "                             canopyWettingFactor, \n",
    "                             thetasat, \n",
    "                             thetares, \n",
    "                             laiscale, \n",
    "                             rootdepth,\n",
    "                             vp_air, \n",
    "                             lai, \n",
    "                             surf_sm,\n",
    "                             avg_temp, \n",
    "                             surf_temp, \n",
    "                             transpirable, \n",
    "                             swe\n",
    "                            ]).T \n",
    "    \n",
    "    ebc       = -(in_ds['Qle_cor'].values + in_ds['Qh_cor'].values)\n",
    "    train_output = np.vstack([in_ds['Qle_cor'].values / 500,\n",
    "                              in_ds['Qh_cor'].values / 500,\n",
    "                              ebc / 500]).T\n",
    "\n",
    "    \n",
    "    if use_mask:\n",
    "        train_input = train_input[mask == 0]\n",
    "        train_output = train_output[mask == 0]    \n",
    "    return train_input.astype(np.float32), train_output.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_input = []\n",
    "all_valid_input = []\n",
    "all_train_output = []\n",
    "all_valid_output = []\n",
    "\n",
    "for i in range(kfold_train_sites.shape[0]):\n",
    "    print(i)\n",
    "    # -----------------------------------------------\n",
    "    # load in data, transform, and split for training\n",
    "    # -----------------------------------------------\n",
    "    train_set = kfold_train_sites[i]\n",
    "    train_data = [etl_single_site(site_dict[s], site_attr[s], site_outp[s], site_parm[s]) for s in train_set]\n",
    "\n",
    "    train_input = np.vstack([td[0] for td in train_data])\n",
    "    train_output = np.vstack([td[1] for td in train_data])\n",
    "    \n",
    "    index_shuffle = np.arange(train_output.shape[0])\n",
    "    np.random.shuffle(index_shuffle)\n",
    "    \n",
    "    train_input = train_input[index_shuffle, :]\n",
    "    train_output = train_output[index_shuffle, :]\n",
    "    \n",
    "    validation_frac = 0.2\n",
    "    validation_start_idx = int(train_output.shape[0] * (1-validation_frac))\n",
    "\n",
    "    train_input, valid_input = train_input[0:validation_start_idx, :], train_input[validation_start_idx:, :]\n",
    "    train_output, valid_output = train_output[0:validation_start_idx], train_output[validation_start_idx:]\n",
    "    \n",
    "    assert np.isnan(train_input).sum() + np.isnan(train_output).sum() == 0\n",
    "    all_train_input.append(train_input)\n",
    "    all_valid_input.append(valid_input)\n",
    "    all_train_output.append(train_output)\n",
    "    all_valid_output.append(valid_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRHistory(keras.callbacks.Callback):\n",
    "    \"\"\"Simple callback for recording the learning rate curve\"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.lr = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.lr.append(model.optimizer._decayed_lr(np.float32).numpy())       \n",
    "        \n",
    "        \n",
    "def mse_eb(y_true, y_pred):\n",
    "    # Normal MSE loss\n",
    "    mse = K.mean(K.square(y_true[:, 0:2]-y_pred[:, 0:2]), axis=-1)\n",
    "    # Loss that penalizes differences between sum(predictions) and sum(true) (energy balance constraint)\n",
    "    sum_constraint = K.mean(K.square(K.sum(y_pred[:, 0:2], axis=-1) + y_true[:, 2] )) / 10\n",
    "    return mse + sum_constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hist = []\n",
    "\n",
    "for i in range(kfold_train_sites.shape[0]):\n",
    "    # -----------------------------------------------\n",
    "    # load in data, transform, and split for training\n",
    "    # -----------------------------------------------\n",
    "    train_input = all_train_input[i]\n",
    "    train_output = all_train_output[i]\n",
    "    valid_input = all_valid_input[i]\n",
    "    valid_output = all_valid_output[i]\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    # Define model hyperparameters\n",
    "    # -----------------------------------------------\n",
    "    loss = mse_eb\n",
    "    activation = 'tanh'    \n",
    "    width = 48\n",
    "    dropout_rate = 0.1\n",
    "    epochs = 200\n",
    "    batch_size = 48 * 7\n",
    "    learning_rate = 1.25e-2\n",
    "    decay_rate = learning_rate / (epochs * epochs)\n",
    "    optimizer = keras.optimizers.SGD(momentum=0.8, learning_rate=learning_rate, decay=decay_rate)\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Define model structure\n",
    "    # -----------------------------------------------\n",
    "    model = keras.Sequential([\n",
    "            layers.Dense(width, activation=activation, input_shape=train_input[0].shape),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(width, activation=activation),\n",
    "            layers.Dense(width, activation=activation),\n",
    "            layers.Dense(width, activation=activation),\n",
    "            layers.Dense(width, activation=activation),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(width, activation=activation),\n",
    "            layers.Dense(3, activation='linear')\n",
    "        ])     \n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Train the model\n",
    "    # -----------------------------------------------\n",
    "    history = model.fit(\n",
    "        train_input, train_output,\n",
    "        validation_data=(valid_input, valid_output),\n",
    "        batch_size=batch_size, epochs=epochs, shuffle=True, verbose=0, \n",
    "        callbacks=[TqdmCallback(verbose=1), EarlyStopping(monitor='val_loss', patience=10), LRHistory()])\n",
    "    all_hist.append(history)\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    # Save the model and history\n",
    "    # -----------------------------------------------\n",
    "    # save the history     \n",
    "    hist_df = pd.DataFrame(history.history) \n",
    "    hist_csv_file = f'../models/history_states_qle_{i}.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "    \n",
    "    # save the model    \n",
    "    model.save(f'../models/train_states_qle_set_{i}.h5')\n",
    "    from KerasWeightsProcessing.convert_weights import txt_to_h5, h5_to_txt\n",
    "    h5_to_txt(\n",
    "        weights_file_name=f'../models/train_states_qle_set_{i}.h5', \n",
    "        output_file_name=f'../models/train_states_qle_set_{i}.txt'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lr_hist.lr, 'o-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (18, 12)\n",
    "for history in all_hist:\n",
    "    plt.plot(history.history['loss'][:], linewidth=3, color='tomato')\n",
    "    plt.plot(history.history['val_loss'][:], linewidth=3, color='olivedrab')\n",
    "    \n",
    "plt.plot(all_hist[-1].history['loss'][:], label='training', linewidth=3, color='tomato')\n",
    "plt.plot(all_hist[-1].history['val_loss'][:], label='validation', linewidth=3, color='olivedrab')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(kfold_test_sites[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'AU-Wac'#kfold_test_sites[0][2]\n",
    "new_input, new_output = etl_single_site(site_dict[site], site_attr[site], site_outp[site], site_parm[site])\n",
    "Qle_obs = site_dict[site]['Qle_cor'].values\n",
    "Qh_obs = site_dict[site]['Qh_cor'].values\n",
    "netrad = site_dict[site]['SWRadAtm'].values + site_dict[site]['LWRadAtm'].values\n",
    "mask = site_dict[site]['gap_filled'].values\n",
    "\n",
    "y_train = model.predict(new_input)\n",
    "#le_pred = 300 * (np.power(np.arctanh(y_train[:, 0])/10.0 + np.cbrt(10.0), 3) - 10.0)\n",
    "#h_pred = 300 * (np.power(np.arctanh(y_train[:, 1])/10.0 + np.cbrt(10.0), 3) - 10.0)\n",
    "le_pred = 500 * y_train#[:, 0]\n",
    "#h_pred = 500 * y_train[:, 1]\n",
    "\n",
    "site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = -1\n",
    "plt.plot(Qle_obs[mask==0], color='darkslategrey', label='FluxNet')\n",
    "plt.plot(le_pred, color='crimson', label='NeuralNet', alpha=0.7)\n",
    "plt.title(f'Number of hidden layers: {n_hidden}')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(r'Latent Heat ($W/m^2$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(Qle_obs[mask==0].flatten()).rolling(7 * 48).mean().plot(color='darkslategrey', linewidth=3, label='Observed')\n",
    "pd.Series(le_pred.flatten()).rolling(7 * 48).mean().plot(color='crimson', linewidth=3, label='Neural Net')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(r'7 Average Latent Heat ($W/m^2$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "axes[0].hist(Qle_obs[mask == 0], bins=20, alpha=0.55, label='FluxNet Observed')\n",
    "axes[0].hist(le_pred, bins=20, color='orange', alpha=0.7, label='NN Output')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('Latent Heat')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "axes[1].scatter(Qle_obs[mask == 0], le_pred, marker='.', alpha=1/510)\n",
    "axes[1].plot([600, -100], [600, -100], color='black', linewidth=3)\n",
    "axes[1].set_xlabel('FluxNet Observed')\n",
    "axes[1].set_ylabel('NN Output')\n",
    "plt.suptitle('Latent heat performance comparison', y=1.02)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "plt.plot(Qle_obs[mask==0][n* (20* 48):20*48*(1+n)], color='grey', label='FluxNet')\n",
    "plt.plot(le_pred[n *(20* 48):20*48*(1+n)], color='crimson', label='NeuralNet')\n",
    "plt.title(f'Number of hidden layers: {n_hidden}')\n",
    "plt.axhline(0, color='black')\n",
    "plt.legend()\n",
    "plt.xlabel('Day of simulation')\n",
    "plt.ylabel('Latent Heat (W/m-2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
