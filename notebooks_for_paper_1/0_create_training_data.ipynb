{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%run ../lib/preprocess_fluxnet.py\n",
    "%run ../lib/classify_soil.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import intake\n",
    "import pandas as pd\n",
    "from dask.distributed import Client\n",
    "import seaborn as sns\n",
    "from numba import jit\n",
    "import matplotlib as mpl\n",
    "import xarray as xr\n",
    "import warnings\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "user = os.environ['USER']\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "contact = 'andrbenn@uw.edu'\n",
    "NPROCS=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_ds = xr.open_dataset('../summa_setup_template_3/forcings/test.nc')\n",
    "for site in tqdm(all_sites):\n",
    "    with xr.open_dataset(f'../../fluxnet/netcdf_processed/{site}.nc') as test_ds:\n",
    "        ds = test_ds.load()\n",
    "    ds['data_step'] = template_ds['data_step']\n",
    "    ds['data_step'].values = 1800.\n",
    "    ds['hruId'] = template_ds['hruId']\n",
    "    ds['time'].encoding['dtype'] = dtype('float64')\n",
    "    ds['time'].encoding['units'] = 'hours since 1990-01-01'\n",
    "    ds.to_netcdf(f'../../fluxnet/netcdf_processed/{site}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=NPROCS, threads_per_worker=1)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in data and merge together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = intake.Catalog('../fluxnet/fluxnet_data/catalog.yml')\n",
    "era_cat = intake.Catalog('../fluxnet/erai_data/catalog.yml')\n",
    "all_site_meta = pd.read_excel(\"../fluxnet/fluxnet_data/FLX_AA-Flx_BIF_LATEST.xlsx\").set_index(\n",
    "    [\"SITE_ID\", \"VARIABLE\"]\n",
    ")[\"DATAVALUE\"]\n",
    "\n",
    "# Sites selected for study\n",
    "all_sites =  ['BE-Vie', 'RU-Fyo', 'CA-Qfo', 'BE-Lon', 'US-Prr', 'NL-Hor',\n",
    "              'IT-MBo', 'IT-Tor', 'IT-SRo', 'AU-Cpr', 'AT-Neu', 'ES-LJu',\n",
    "              'US-NR1', 'US-Var', 'US-Los', 'FI-Hyy', 'CA-TP3', 'DE-Hai',\n",
    "              'DE-Gri', 'FI-Let', 'CZ-wet', 'DK-Eng', 'DE-Tha', 'US-Whs',\n",
    "              'CA-TPD', 'IT-Lav', 'FR-LBr', 'US-KS2', 'US-Goo', 'US-WCr',\n",
    "              'US-IB2', 'CA-Gro', 'IT-Noe', 'US-Blo', 'AU-Wac', 'AU-Wom',\n",
    "              'CH-Cha', 'AU-ASM', 'DE-Kli', 'US-Ton', 'FI-Sod', 'CA-TP1',\n",
    "              'DE-Obe', 'US-CRT', 'AU-DaS', 'IT-Cpz', 'US-Syv', 'IT-Ro2',\n",
    "              'FR-Pue', 'DE-Geb', 'US-AR2', 'AU-How', 'US-GLE', 'AU-Stp',\n",
    "              'IT-Ren', 'ES-Amo', 'CH-Fru', 'FI-Jok', 'CN-HaM', 'US-ARM']\n",
    "\n",
    "# Filter out sites missing half hourly data\n",
    "for site in all_sites:\n",
    "    if not len(glob(f'../../fluxnet/fluxnet_data/FLX_{site}_FLUXNET2015_FULLSET_HH_*.csv')):\n",
    "        all_sites.remove(site)\n",
    "\n",
    "all_site_meta = all_site_meta.loc[all_sites]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_fluxnet(cat, all_site_meta)\n",
    "all_sites = list(df.index.get_level_values(0).unique())\n",
    "era_df = load_era(era_cat, all_site_meta)\n",
    "era_sites = list(era_df.index.get_level_values(0).unique())\n",
    "all_sites = list(set(all_sites).intersection(set(era_sites)))\n",
    "merged = {}\n",
    "for s in tqdm(all_sites):\n",
    "    merged[s] = merge_fluxnet_era(df.loc[s], era_df.loc[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebc_filter = ~df['LE_CORR'].isna()\n",
    "ml_df = df[ebc_filter]\n",
    "ml_df.to_csv('../data/ml_summa_all_training.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter data for SUMMA runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "out = get_longest_sequence(df, all_sites, n_workers=NPROCS, min_length=3*17520, good_frac=0.85)\n",
    "selected_sites = np.unique(out.index.get_level_values(0))\n",
    "test_dfs = [out.loc[selected_sites[i]] for i in range(len(selected_sites))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_forcings(site, test_df):\n",
    "    template_ds = xr.open_dataset('../../fluxnet/summa_setup_template_3/forcings/test.nc')\n",
    "    raw_df = test_df\n",
    "    filled_df = gap_fill(raw_df)\n",
    "    raw_ds = to_summa_ds(filled_df)\n",
    "    \n",
    "    file_attrs = {}\n",
    "    file_attrs['Site name'] = site\n",
    "    file_attrs['Contact'] = contact\n",
    "    file_attrs['Production time'] = time.ctime()\n",
    "    \n",
    "    attrs_ds = populate_metadata(raw_ds, file_attrs)\n",
    "    attrs_ds['data_step'] = template_ds['data_step']\n",
    "    attrs_ds['data_step'].values = 1800.\n",
    "    attrs_ds['hruId'] = template_ds['hruId']\n",
    "    attrs_ds['time'].encoding['dtype'] = dtype('float64')\n",
    "    attrs_ds['time'].encoding['units'] = 'minutes since 2000-01-01'\n",
    "    \n",
    "    attrs_ds.to_netcdf(f'../netcdf_processed/{site}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parallel(n_jobs=NPROCS)(delayed(write_forcings)(s, test_dfs[i]) for i, s in enumerate(selected_sites));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_ds = xr.open_dataset('../summa_setup_template_3/forcings/test.nc')\n",
    "for site in tqdm(all_sites):\n",
    "    with xr.open_dataset(f'../../fluxnet/netcdf_processed/{site}.nc') as test_ds:\n",
    "        ds = test_ds.load()\n",
    "    ds['data_step'] = template_ds['data_step']\n",
    "    ds['data_step'].values = 1800.\n",
    "    ds['hruId'] = template_ds['hruId']\n",
    "    ds['time'].encoding['dtype'] = dtype('float64')\n",
    "    ds['time'].encoding['units'] = 'hours since 1990-01-01'\n",
    "    ds.to_netcdf(f'../../fluxnet/netcdf_processed/{site}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in tqdm(all_sites):\n",
    "    test_ds = xr.open_dataset(f'../../fluxnet/netcdf_processed/{site}.nc')\n",
    "    start = pd.to_datetime(test_ds.time.values[0]).strftime('%Y-%m-%d %H:%M')\n",
    "    finsh = pd.to_datetime(test_ds.time.values[-1]).strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "    script = f\"\"\"\n",
    "    mkdir -p ../sites\n",
    "    cp -r ../summa_setup_template_3 ../sites/{site}\n",
    "    \n",
    "    # Move forcing files\n",
    "    cp -r ../../fluxnet/netcdf_processed/{site}.nc ../sites/{site}/forcings/\n",
    "    sed -i \"s|test.nc|{site}.nc|g\" ../sites/{site}/forcings/forcing_file_list.txt\n",
    "    \n",
    "    # Move attribute files\n",
    "    cp -r ../../fluxnet/local_attrs_processed/{site}_local_attrs.nc ../sites/{site}/params/local_attributes.nc\n",
    "    cp -r ../../fluxnet/trial_params_processed/{site}_trial_params.nc ../sites/{site}/params/parameter_trial.nc\n",
    "    \n",
    "    \n",
    "    # Set output filename template and replace filename\n",
    "    cd ../sites/{site}\n",
    "    # Set start and finish times\n",
    "    sed -i \"s|simStartTime.*|simStartTime    '{start}'   ! simulation start time|g\" ./template_file_manager.txt\n",
    "    sed -i \"s|simEndTime.*|simEndTime    '{finsh}'   ! simulation end time|g\" ./template_file_manager.txt\n",
    "    ./install_local_setup.sh\n",
    "    cd - \n",
    "    \"\"\"\n",
    "    retval = os.system(script)\n",
    "    assert retval == 0, site"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
