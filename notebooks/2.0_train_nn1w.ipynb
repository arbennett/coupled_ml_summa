{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NN to generate total latent heat given only forcing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "from IPython.display import SVG\n",
    "from tqdm.keras import TqdmCallback\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import plot_model, model_to_dot\n",
    "from tensorflow.keras import layers\n",
    "import dask.dataframe as dd\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "sns.set_context('talk')\n",
    "mpl.style.use('seaborn-bright')\n",
    "mpl.rcParams['figure.figsize'] = (18, 12)\n",
    "def cube(x):\n",
    "    return np.power(x, 3)\n",
    "\n",
    "#strategy = tf.distribute.MirroredStrategy()\n",
    "#print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "dtype='float32'\n",
    "K.set_floatx(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = os.listdir('../sites')\n",
    "bad_sites = []\n",
    "sim_sites = [s for s in sites if s not in bad_sites]\n",
    "\n",
    "seed = 50334\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(sim_sites)\n",
    "len(sim_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_dict = {s: xr.open_dataset(f'../sites/{s}/forcings/{s}.nc').isel(hru=0, drop=True).load() \n",
    "             for s in sim_sites}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_attr = {s: xr.open_dataset(f'../sites/{s}/params/local_attributes.nc').isel(hru=0, drop=True).load() \n",
    "             for s in sim_sites}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_forc = {s: xr.open_dataset(f'../sites/{s}/output/template_output_{s}_timestep.nc').isel(hru=0, drop=True).load() \n",
    "             for s in sim_sites}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_parm = {s: xr.open_dataset(f'../sites/{s}/params/parameter_trial.nc').isel(hru=0, drop=True).load() \n",
    "             for s in sim_sites}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfold = 4\n",
    "kfold_test_sites = np.array(sim_sites).reshape(nfold, -1)\n",
    "\n",
    "kfold_train_sites = np.vstack([\n",
    "    list(set(sim_sites) - set(test_sites)) for test_sites in kfold_test_sites\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_single_site(in_ds, attr_ds, parm_ds, use_mask=True):\n",
    "   \n",
    "    #---------------------------------------------------------------------------\n",
    "    # Forcings\n",
    "    #---------------------------------------------------------------------------\n",
    "    airtemp   = (((in_ds['airtemp'].values / 27.315) - 10) / 2) + 0.5\n",
    "    spechum   = (in_ds['spechum'].values * 50)  \n",
    "    swradatm  = np.cbrt((in_ds['SWRadAtm'].values / 1000) )\n",
    "    lwradatm  = in_ds['LWRadAtm'].values / (2 * 273.16)\n",
    "    pptrate   = 10 * np.cbrt(in_ds['pptrate'].values)\n",
    "    airpres   = (10 - (in_ds['airpres'].values / 10132.5)) / 2\n",
    "    windspd   = in_ds['windspd'].values / 10\n",
    "    mask      = in_ds['gap_filled'].values\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # Parameters\n",
    "    #---------------------------------------------------------------------------\n",
    "    soiltype = attr_ds['soilTypeIndex'].values[()] * np.ones_like(mask) / 12\n",
    "    vegtype = attr_ds['vegTypeIndex'].values[()] * np.ones_like(mask) / 12\n",
    "\n",
    "    canheight = parm_ds['heightCanopyTop'].values[()] * np.ones_like(mask) / 30\n",
    "    vcmax = parm_ds['vcmax_Kn'].values[()] * np.ones_like(mask)\n",
    "    canopyWettingFactor = parm_ds['canopyWettingFactor'].values[()] * np.ones_like(mask)\n",
    "    thetasat = parm_ds['theta_sat'].values[()] * np.ones_like(mask)\n",
    "    thetares = parm_ds['theta_res'].values[()] * np.ones_like(mask)\n",
    "    laiscale = parm_ds['laiScaleParam'].values[()] * np.ones_like(mask) / 3\n",
    "    rootdepth = parm_ds['rootingDepth'].values[()] * np.ones_like(mask) / 5\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # Arrange inputs and outputs\n",
    "    #---------------------------------------------------------------------------    \n",
    "    train_input = np.vstack([airtemp, spechum, swradatm, \n",
    "                             lwradatm, pptrate, airpres, \n",
    "                             windspd, vegtype, soiltype,\n",
    "                             canheight, vcmax, canopyWettingFactor, \n",
    "                             thetasat, thetares, laiscale, rootdepth\n",
    "                            ]).T \n",
    "    ebc       = -(in_ds['Qle_cor'].values + in_ds['Qh_cor'].values)\n",
    "    train_output = np.vstack([in_ds['Qle_cor'].values / 500,\n",
    "                              in_ds['Qh_cor'].values / 500,\n",
    "                              ebc / 500]).T\n",
    "    \n",
    "    if use_mask:\n",
    "        train_input = train_input[mask == 0]\n",
    "        train_output = train_output[mask == 0]    \n",
    "    return train_input.astype(np.float32), train_output.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_input = []\n",
    "all_valid_input = []\n",
    "all_train_output = []\n",
    "all_valid_output = []\n",
    "\n",
    "for i in range(kfold_train_sites.shape[0]):\n",
    "    print(i)\n",
    "    # -----------------------------------------------\n",
    "    # load in data, transform, and split for training\n",
    "    # -----------------------------------------------\n",
    "    train_set = kfold_train_sites[i]\n",
    "    train_data = [etl_single_site(site_dict[s], site_attr[s], site_parm[s]) for s in train_set]\n",
    "\n",
    "    train_input = np.vstack([td[0] for td in train_data])\n",
    "    train_output = np.vstack([td[1] for td in train_data])\n",
    "    \n",
    "    index_shuffle = np.arange(train_output.shape[0])\n",
    "    np.random.shuffle(index_shuffle)\n",
    "    \n",
    "    train_input = train_input[index_shuffle, :]\n",
    "    train_output = train_output[index_shuffle, :]\n",
    "    \n",
    "    validation_frac = 0.2\n",
    "    validation_start_idx = int(train_output.shape[0] * (1-validation_frac))\n",
    "\n",
    "    train_input, valid_input = train_input[0:validation_start_idx, :], train_input[validation_start_idx:, :]\n",
    "    train_output, valid_output = train_output[0:validation_start_idx], train_output[validation_start_idx:]\n",
    "    \n",
    "    assert np.isnan(train_input).sum() + np.isnan(train_output).sum() == 0\n",
    "    all_train_input.append(train_input)\n",
    "    all_valid_input.append(valid_input)\n",
    "    all_train_output.append(train_output)\n",
    "    all_valid_output.append(valid_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRHistory(keras.callbacks.Callback):\n",
    "    \"\"\"Simple callback for recording the learning rate curve\"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.lr = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.lr.append(model.optimizer._decayed_lr(np.float32).numpy())          \n",
    "        \n",
    "def mse_eb(y_true, y_pred):\n",
    "    # Normal MSE loss\n",
    "    mse = K.mean(K.square(y_true[:, 0:2]-y_pred[:, 0:2]), axis=-1)\n",
    "    # Loss that penalizes differences between sum(predictions) and sum(true) (energy balance constraint)\n",
    "    sum_constraint = K.mean(K.square(K.sum(y_pred[:, 0:2], axis=-1) + y_true[:, 2] )) / 10\n",
    "    return mse + sum_constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hist = []\n",
    "\n",
    "for i in range(kfold_train_sites.shape[0]):\n",
    "    # -----------------------------------------------\n",
    "    # load in data, transform, and split for training\n",
    "    # -----------------------------------------------\n",
    "    train_input = all_train_input[i]\n",
    "    train_output = all_train_output[i]\n",
    "    valid_input = all_valid_input[i]\n",
    "    valid_output = all_valid_output[i]\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    # Define model hyperparameters\n",
    "    # -----------------------------------------------\n",
    "\n",
    "    \n",
    "    loss = mse_eb\n",
    "    activation = 'tanh'    \n",
    "    width = 48\n",
    "    dropout_rate = 0.1\n",
    "    epochs = 200\n",
    "    batch_size = 48 * 7\n",
    "    learning_rate = 1.25e-2\n",
    "    decay_rate = learning_rate / (epochs * epochs)\n",
    "    optimizer = keras.optimizers.SGD(momentum=0.8, learning_rate=learning_rate, decay=decay_rate)\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Define model structure\n",
    "    # -----------------------------------------------\n",
    "    model = keras.Sequential([\n",
    "            layers.Dense(width, activation=activation, input_shape=train_input[0].shape),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(width, activation=activation),\n",
    "            layers.Dense(width, activation=activation),\n",
    "            layers.Dense(width, activation=activation),\n",
    "            layers.Dense(width, activation=activation),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(width, activation=activation),\n",
    "            layers.Dense(3, activation='linear')\n",
    "        ])     \n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Train the model\n",
    "    # -----------------------------------------------\n",
    "    history = model.fit(\n",
    "        train_input, train_output,\n",
    "        validation_data=(valid_input, valid_output),\n",
    "        batch_size=batch_size, epochs=epochs, shuffle=True, verbose=0, \n",
    "        callbacks=[TqdmCallback(verbose=1), EarlyStopping(monitor='val_loss', patience=5), LRHistory()])\n",
    "    all_hist.append(history)\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    # Save the model and history\n",
    "    # -----------------------------------------------\n",
    "    # save the history     \n",
    "    hist_df = pd.DataFrame(history.history) \n",
    "    hist_csv_file = f'../models/history_fluxes_qle_{i}.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "    \n",
    "    # save the model    \n",
    "    model.save(f'../models/train_fluxes_qle_set_{i}.h5')\n",
    "    from KerasWeightsProcessing.convert_weights import txt_to_h5, h5_to_txt\n",
    "    h5_to_txt(\n",
    "        weights_file_name=f'../models/train_fluxes_qle_set_{i}.h5', \n",
    "        output_file_name=f'../models/train_fluxes_qle_set_{i}.txt'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
