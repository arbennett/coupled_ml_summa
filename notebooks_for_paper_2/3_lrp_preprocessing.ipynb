{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, loading, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "from IPython.display import SVG\n",
    "from functools import partial\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, Point, MultiPolygon\n",
    "\n",
    "from sklearn import linear_model\n",
    "import pysumma.plotting as psp\n",
    "import pysumma.utils as psu\n",
    "import pysumma.evaluation as pse\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import plot_model, model_to_dot\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import shap\n",
    "import innvestigate\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ['TF_NUM_INTEROP_THREADS'] = '1'\n",
    "os.environ['TF_NUM_INTRAOP_THREADS'] = '1'\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "mpl.style.use('seaborn-talk')\n",
    "sns.set_context('talk')\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "\n",
    "K.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('../new_models/all_var_dense_dropout.h5')\n",
    "sim_sites = os.listdir('../sites')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_eb(y_true, y_pred):\n",
    "    # Normal MSE loss\n",
    "    mse = K.mean(K.square(y_true[:, 0:2]-y_pred[:, 0:2]), axis=-1)\n",
    "    # Loss that penalizes differences between sum(predictions) and sum(true) (energy balance constraint)\n",
    "    sum_constraint = K.mean(K.square(K.sum(y_pred[:, 0:2], axis=-1) + y_true[:, 2] )) / 10\n",
    "    return mse + sum_constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_strings = ['airtemp',       # 0\n",
    "                   'relhum',        # 1\n",
    "                   'swradatm',      # 2\n",
    "                   'surf_sm',       # 3\n",
    "                   'lai',           # 4\n",
    "                   'vegtype',       # 5\n",
    "              ]           \n",
    "\n",
    "var_to_color = {\n",
    "    'swradatm': '#4363d8',\n",
    "    'airtemp': '#e6194B',\n",
    "    'relhum': '#f58231',\n",
    "    'surf_sm': '#911eb4',\n",
    "    'lai': '#9A6324',\n",
    "    'vegtype': '#469990',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_dict = {s: xr.open_dataset(f'../sites/{s}/forcings/{s}.nc').isel(hru=0, drop=True).load() \n",
    "             for s in sim_sites}\n",
    "site_attr = {s: xr.open_dataset(f'../sites/{s}/params/local_attributes.nc').isel(hru=0, drop=True).load() \n",
    "             for s in sim_sites}\n",
    "\n",
    "site_outp = {s: xr.open_dataset(f'../prepped_output_for_casper/lrp_nn_output_{s}_timestep.nc').isel(hru=0, drop=True).load() \n",
    "             for s in sim_sites}\n",
    "\n",
    "site_parm = {s: xr.open_dataset(f'../sites/{s}/params/parameter_trial.nc').isel(hru=0, drop=True).load() \n",
    "             for s in sim_sites}\n",
    "\n",
    "site_sa = {s: xr.open_dataset(f'../sites/{s}/output/template_output_{s}_timestep.nc').isel(hru=0, drop=True).load() \n",
    "             for s in sim_sites}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_time(sim, obs, roundto='min'):\n",
    "    sim['time'] = sim['time'].dt.round(roundto)\n",
    "    obs['time'] = obs['time'].dt.round(roundto)\n",
    "    sim_start = sim['time'].values[1]\n",
    "    sim_stop = sim['time'].values[-2]\n",
    "    obs_start = obs['time'].values[1]\n",
    "    obs_stop = obs['time'].values[-2]\n",
    "    start = max(sim_start, obs_start)\n",
    "    stop = min(sim_stop, obs_stop)\n",
    "    return slice(start, stop)\n",
    "\n",
    "def calc_relhum(T, p, sh):\n",
    "    T0 = 273.16\n",
    "    return sh * (0.263*p)  / np.exp((17.67*(T-T0))/(T-29.65))\n",
    "\n",
    "def L_vap(T):\n",
    "    # J / g\n",
    "    C = T - 273.16\n",
    "    return 2500.8 - (2.36 * C) + (0.0016 * C**2) - (0.00006 * C**3)\n",
    "\n",
    "def Qle_to_ET(Q, T):\n",
    "    g_per_kg = 1e3\n",
    "    return (1 / (g_per_kg * L_vap(T))) * Q\n",
    "\n",
    "def calc_pet(tmin, tmax, tmean, rad, rad_mult=1):\n",
    "    return 0.0023 * (tmean + 17.8) * (tmax - tmin) ** 0.5 * 0.408 * (rad * rad_mult)\n",
    "\n",
    "def etl_single_site(ds, use_mask=True):\n",
    "   \n",
    "    airtemp   = (((ds['airtemp'].values / 27.315) - 10) / 2) + 0.5\n",
    "    swradatm  = (ds['SWRadAtm'].values / 1000) \n",
    "    mask      = ds['gap_filled'].values\n",
    "    relhumid  = calc_relhum(ds['airtemp'].values, ds['airpres'].values, ds['spechum']) / 100\n",
    "    relhumid[relhumid<0] = 0\n",
    "    lai = ds['scalarLAI'].values / 12\n",
    "    try:\n",
    "        canwidth = (ds['heightCanopyTop']).values / 20\n",
    "    except:\n",
    "        canwidth = 1/(lai + 0.001)\n",
    "    \n",
    "    vegtype = ds['vegTypeIndex'].values[()] * np.ones_like(mask) / 12\n",
    "\n",
    "    thetasat = ds['theta_sat'].values[()] * np.ones_like(mask)\n",
    "    fieldcapacity = ds['fieldCapacity'].values[()] * np.ones_like(mask)\n",
    "    soilwilting = ds['critSoilWilting'].values[()] * np.ones_like(mask)\n",
    "    sm_min = soilwilting\n",
    "      \n",
    "    # Surface moisture\n",
    "    nlayers_top = 0\n",
    "    nlayers_bot = 4\n",
    "    surf_idx = -len(ds.midSoil)\n",
    "    surf_sm = ds['mLayerVolFracWat'].copy(deep=True)\n",
    "    vmask = surf_sm != -9999\n",
    "    \n",
    "    depth = ds['mLayerHeight'].copy(deep=True)\n",
    "    dmask = depth != -9999\n",
    "    depth.values = psp.utils.justify(depth.where(dmask).values)\n",
    "    depth = depth.isel(midToto=slice(surf_idx+nlayers_top, surf_idx+nlayers_bot))\n",
    "    depth = depth / depth.sum(dim='midToto')\n",
    "    \n",
    "    surf_sm.values = psp.utils.justify(surf_sm.where(vmask).values)\n",
    "    surf_sm = surf_sm.isel(midToto=slice(surf_idx+nlayers_top, surf_idx+nlayers_bot))\n",
    "    surf_sm *= depth\n",
    "    surf_sm = surf_sm.sum(dim='midToto')\n",
    "    surf_sm = (surf_sm - sm_min[0]) / (thetasat[0] - sm_min[0])\n",
    "\n",
    "    train_input = np.vstack([airtemp,       # 0\n",
    "                             relhumid,      # 1\n",
    "                             swradatm,      # 2\n",
    "                             surf_sm,       # 3\n",
    "                             lai * canwidth, # 4\n",
    "                             vegtype,       # 5\n",
    "                            ]).T \n",
    "    \n",
    "    train_output = np.vstack([ds['Qle_cor'].values / 500,\n",
    "                              ds['Qh_cor'].values / 500,]).T\n",
    "    \n",
    "    if use_mask:\n",
    "        train_input = train_input[mask == 0]\n",
    "        train_output = train_output[mask == 0]    \n",
    "    return train_input.astype(np.float32), train_output.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_variables(ds):\n",
    "    \n",
    "    new_input, new_output = etl_single_site(ds, use_mask=False)\n",
    "    t = ds['time'].values\n",
    "    \n",
    "    ds['transpirable'] = ds['scalarLatHeatTotal'].copy(deep=True)\n",
    "    ds['transpirable'].values = new_input[:, 5]\n",
    "    ds['surf_sm'] = ds['scalarLatHeatTotal'].copy(deep=True)\n",
    "    ds['surf_sm'].values = new_input[:, 3]\n",
    "    \n",
    "    ds['scalarLatHeatTotal'].values = (500 * model.predict(new_input)[:, 0].flatten())\n",
    "    ds['scalarSenHeatTotal'].values = (500 * model.predict(new_input)[:, 1].flatten())\n",
    "    \n",
    "    ds['halfhourofday'] = ds['scalarLatHeatTotal'].copy(deep=True)\n",
    "    ds['halfhourofday'].values = (2 * ds['time'].dt.hour + ds['time'].dt.minute // 30)\n",
    "    \n",
    "    ds['day'] = (ds['halfhourofday'] == 0).cumsum()\n",
    "    \n",
    "    return ds\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrp_to_ds(r, timedim, feature_strings=feature_strings):\n",
    "    feature_das = []\n",
    "    for i, feature in enumerate(feature_strings):\n",
    "            r_feat = r[:, i]\n",
    "            feature_das.append(xr.DataArray(r_feat, coords={'time': timedim}, dims=['time'], name=feature))\n",
    "    r_ds = xr.merge(feature_das) \n",
    "    in_sum = np.sum(r)\n",
    "    out_sum = np.sum(r_ds.sum().to_array())\n",
    "    return r_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model(new_input, time_dim, analyzer):\n",
    "    \n",
    "    r_qle = analyzer.analyze(new_input, neuron_selection=0)\n",
    "    r_qh = analyzer.analyze(new_input, neuron_selection=1)\n",
    "    \n",
    "    feature_strings = ['airtemp', 'relhum', 'swradatm', 'surf_sm', 'lai', 'vegtype']\n",
    "    r_qle_ds = lrp_to_ds(r_qle, time_dim, feature_strings)\n",
    "    r_qh_ds = lrp_to_ds(r_qh, time_dim, feature_strings)\n",
    "    return r_qle_ds, r_qh_ds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(simvar, obsvar, metric=pse.nash_sutcliffe_efficiency):\n",
    "    sv = simvar.copy(deep=True)\n",
    "    ov = obsvar.copy(deep=True)\n",
    "    t = pse.trim_time(sv, ov)\n",
    "    return metric(sv.sel(time=t), ov.sel(time=t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sim_sites:\n",
    "    # trim times to match\n",
    "    in_ds = site_dict[s]\n",
    "    outp_ds = site_outp[s]\n",
    "    ts = trim_time(in_ds, outp_ds)\n",
    "    in_ds = in_ds.sel(time=ts)\n",
    "    in_ds['relhum'] = calc_relhum(in_ds['airtemp'], in_ds['airpres'], in_ds['spechum'])\n",
    "    outp_ds = outp_ds.sel(time=ts)\n",
    "    site_dict[s] = in_ds\n",
    "    site_outp[s] = outp_ds\n",
    "    site_nn1w[s] = site_nn1w[s].sel(time=ts)\n",
    "    site_sa[s] = site_sa[s].sel(time=ts)\n",
    "    \n",
    "\n",
    "site_ds = {s: modify_variables(xr.merge([site_dict[s], site_attr[s], site_outp[s], site_parm[s]])) for s in tqdm(sim_sites)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing, filtering, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_igbp = pd.read_csv('./VEGPARM_IGBP_MODIS_NOAH.TBL', index_col=-1, skipinitialspace=True)\n",
    "veg_igbp.index = veg_igbp.index.map(lambda x: x.strip().replace(\"'\", \"\"))\n",
    "\n",
    "soil_rosetta = pd.read_csv('./SOILPARM_ROSETTA.TBL', index_col=-1, skipinitialspace=True)\n",
    "soil_rosetta.index = soil_rosetta.index.map(lambda x: x.strip().replace(\"'\", \"\"))\n",
    "\n",
    "data = np.ones((len(sites), 5))\n",
    "for i, site in enumerate(sites):\n",
    "    local_attrs = xr.open_dataset(f'../sites/{site}/params/local_attributes.nc')\n",
    "    data[i, 0] = local_attrs['latitude'].values[0]\n",
    "    data[i, 1] = local_attrs['longitude'].values[0]\n",
    "    data[i, 2] = local_attrs['elevation'].values[0]\n",
    "    data[i, 3] = local_attrs['soilTypeIndex'].values[0]\n",
    "    data[i, 4] = local_attrs['vegTypeIndex'].values[0]\n",
    "site_attrs = gpd.GeoDataFrame(data, index=sites, columns=['Latitude', 'Longitude', 'Elevation', 'Soil Code', 'Veg Code'])\n",
    "site_attrs['geometry'] = [Point(xy) for xy in zip(site_attrs['Longitude'], site_attrs['Latitude'])]\n",
    "veg_idxs = site_attrs['Veg Code'].unique().astype(int)\n",
    "site_attrs['Veg Type'] = site_attrs['Veg Code'].apply(lambda x: veg_igbp.where(veg_igbp['VEGTYPINDEX'] == x).dropna().index[0])\n",
    "site_attrs['Soil Type'] = site_attrs['Soil Code'].apply(lambda x: soil_rosetta.where(soil_rosetta['SOILTYPINDEX']== x).dropna().index[0])\n",
    "\n",
    "veg_types = np.unique(site_attrs['Veg Type'].values)\n",
    "\n",
    "colors = ['#e6194b', '#3cb44b', '#ffe119', \n",
    "          '#4363d8', '#f58231', '#911eb4', \n",
    "          '#46f0f0', '#f032e6', '#bcf60c', \n",
    "          '#fabebe', '#008080', '#e6beff', ]\n",
    "\n",
    "vegcolors = ['goldenrod',  'firebrick',  'grey', \n",
    "          'chocolate',  'orange',  'dodgerblue', \n",
    "          'khaki',  'red', 'lightgreen', \n",
    "          'yellowgreen',  'mediumseagreen',  'forestgreen', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegtypes = [site_attrs.loc[site]['Veg Code'] for site in sim_sites]\n",
    "soiltypes = [site_attrs.loc[site]['Soil Code'] for site in sim_sites]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_etp = {}\n",
    "obs_etp = {}\n",
    "\n",
    "petp = {}\n",
    "pet = {}\n",
    "dailyp = {}\n",
    "\n",
    "for site in sim_sites:\n",
    "    agg_period = '1000D'\n",
    "    sim_qle = -site_outp[site]['scalarLatHeatTotal']\n",
    "    obs_qle = site_dict[site]['Qle_cor']\n",
    "    obs_p = (site_dict[site]['pptrate']).sum().values\n",
    "    \n",
    "    obs_T = site_dict[site]['airtemp']\n",
    "    sim_et = Qle_to_ET(sim_qle, obs_T).sum().values\n",
    "    obs_et = Qle_to_ET(obs_qle, obs_T).sum().values\n",
    "    \n",
    "    sim_etp[site] = np.nanmean(sim_et) / np.nanmean(obs_p)\n",
    "    obs_etp[site] = np.nanmean(obs_et) / np.nanmean(obs_p)\n",
    "\n",
    "    ds = site_dict[site] \n",
    "    daily_p = 1800.0 * ds['pptrate'].resample(time='D').sum()\n",
    "    dailyp[site] = np.nanmean(daily_p)\n",
    "    \n",
    "    # Calculate temperatures in C\n",
    "    seconds_per_half_hour = 1800.0\n",
    "    to_mega = 1e6\n",
    "    kelvin_to_celcius = 273.16\n",
    "    tmean = ds['airtemp'].resample(time='30D').mean() - kelvin_to_celcius\n",
    "    tmin  = ds['airtemp'].resample(time='30D').min()  - kelvin_to_celcius\n",
    "    tmax  = ds['airtemp'].resample(time='30D').max()  - kelvin_to_celcius\n",
    "    \n",
    "    # Aggregate and convert to MJ / day * m^2\n",
    "    netrad  = ds['SWRadAtm'].resample(time='D').sum().resample(time='30D').mean().values\n",
    "    netrad = (netrad * seconds_per_half_hour) / (to_mega )#* 0.72)\n",
    "   \n",
    "    # Use Hargreave's eq to estimate PET, then aggregate to yearly\n",
    "    daily_pet = calc_pet(tmin, tmax, tmean, netrad)\n",
    "    \n",
    "    yearly_pet = daily_pet\n",
    "    yearly_p = daily_p\n",
    "    pet[site] = np.nanmean(yearly_pet)\n",
    "    petp[site] = np.nanmean(yearly_pet) / np.nanmean(yearly_p)\n",
    "\n",
    "    \n",
    "obs_etp = dict(sorted(obs_etp.items(), key=lambda item: item[1]))\n",
    "sim_etp = dict(sorted(sim_etp.items(), key=lambda item: item[1]))\n",
    "    \n",
    "petp = dict(sorted(petp.items(), key=lambda item: item[1]))\n",
    "pet = dict(sorted(pet.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_pet = {}\n",
    "for s in tqdm(sim_sites):\n",
    "    e = obs_etp[s]\n",
    "    p = petp[s]\n",
    "    et_pet[s] = e/p\n",
    "    \n",
    "et_pet = dict(sorted(et_pet.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nn1w_nse_qle = {site: compute_metric(-site_nn1w[site]['scalarLatHeatTotal'], \n",
    "                                 site_ds[site]['Qle_cor']) \n",
    "                  for site in tqdm(sim_sites)}\n",
    "lrp_nse_qle = {site: compute_metric(site_ds[site]['scalarLatHeatTotal'], \n",
    "                                  site_ds[site]['Qle_cor']) \n",
    "                  for site in tqdm(sim_sites)}\n",
    "standalone_nse_qle    = {site: compute_metric(-site_sa[site]['scalarLatHeatTotal'], \n",
    "                                       site_ds[site]['Qle_cor']) \n",
    "                  for site in tqdm(sim_sites)}\n",
    "\n",
    "standalone_kge_qle = {site: compute_metric(-site_sa[site]['scalarLatHeatTotal'], \n",
    "                                       site_ds[site]['Qle_cor'], \n",
    "                                       metric=pse.kling_gupta_efficiency) \n",
    "                  for site in tqdm(sim_sites)}\n",
    "\n",
    "nn1w_kge_qle    = {site: compute_metric(-site_nn1w[site]['scalarLatHeatTotal'], \n",
    "                                       site_ds[site]['Qle_cor'], \n",
    "                                       metric=pse.kling_gupta_efficiency) \n",
    "                  for site in tqdm(sim_sites)}\n",
    "\n",
    "lrp_kge_qle    = {site: compute_metric(site_ds[site]['scalarLatHeatTotal'], \n",
    "                                       site_ds[site]['Qle_cor'], \n",
    "                                       metric=pse.kling_gupta_efficiency) \n",
    "                  for site in tqdm(sim_sites)}\n",
    "\n",
    "\n",
    "\n",
    "nn1w_nse_qh = {site: compute_metric(-site_nn1w[site]['scalarSenHeatTotal'], \n",
    "                                 site_ds[site]['Qh_cor']) \n",
    "                  for site in tqdm(sim_sites)}\n",
    "lrp_nse_qh = {site: compute_metric(site_ds[site]['scalarSenHeatTotal'], \n",
    "                                  site_ds[site]['Qh_cor']) \n",
    "                  for site in tqdm(sim_sites)}\n",
    "standalone_nse_qh    = {site: compute_metric(-site_sa[site]['scalarSenHeatTotal'], \n",
    "                                       site_ds[site]['Qh_cor']) \n",
    "                  for site in tqdm(sim_sites)}\n",
    "\n",
    "standalone_kge_qh = {site: compute_metric(-site_sa[site]['scalarSenHeatTotal'], \n",
    "                                       site_ds[site]['Qh_cor'], \n",
    "                                       metric=pse.kling_gupta_efficiency) \n",
    "                  for site in tqdm(sim_sites)}\n",
    "\n",
    "nn1w_kge_qh    = {site: compute_metric(-site_nn1w[site]['scalarSenHeatTotal'], \n",
    "                                       site_ds[site]['Qh_cor'], \n",
    "                                       metric=pse.kling_gupta_efficiency) \n",
    "                  for site in tqdm(sim_sites)}\n",
    "\n",
    "lrp_kge_qh    = {site: compute_metric(site_ds[site]['scalarSenHeatTotal'], \n",
    "                                       site_ds[site]['Qh_cor'], \n",
    "                                       metric=pse.kling_gupta_efficiency) \n",
    "                  for site in tqdm(sim_sites)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_vals = np.arange(0, len(sim_sites)) / len(sim_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_attrs['ET/PET'] = pd.Series(et_pet)\n",
    "site_attrs['PET'] = pd.Series(pet)\n",
    "site_attrs['PET/P'] = pd.Series(petp)\n",
    "site_attrs['ET/P_obs'] = pd.Series(obs_etp)\n",
    "site_attrs['ET/P_sim'] = pd.Series(sim_etp)\n",
    "\n",
    "site_attrs['KGE_Qle_SA'] = pd.Series(standalone_kge_qle)\n",
    "site_attrs['KGE_Qh_SA'] = pd.Series(standalone_kge_qh)\n",
    "site_attrs['NSE_Qle_SA'] = pd.Series(standalone_nse_qle)\n",
    "site_attrs['NSE_Qh_SA'] = pd.Series(standalone_nse_qh)\n",
    "\n",
    "site_attrs['KGE_Qle_NN1W'] = pd.Series(nn1w_kge_qle)\n",
    "site_attrs['KGE_Qh_NN1W'] = pd.Series(nn1w_kge_qh)\n",
    "site_attrs['NSE_Qle_NN1W'] = pd.Series(nn1w_nse_qle)\n",
    "site_attrs['NSE_Qh_NN1W'] = pd.Series(nn1w_nse_qh)\n",
    "\n",
    "site_attrs['KGE_Qle_NNLRP'] = pd.Series(lrp_kge_qle)\n",
    "site_attrs['KGE_Qh_NNLRP'] = pd.Series(lrp_kge_qh)\n",
    "site_attrs['NSE_Qle_NNLRP'] = pd.Series(lrp_nse_qle)\n",
    "site_attrs['NSE_Qh_NNLRP'] = pd.Series(lrp_nse_qh)\n",
    "\n",
    "site_attrs['Site'] = site_attrs.index\n",
    "site_attrs.to_file(\"../data_for_paper_2/site_attrs.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10, 7))\n",
    "\n",
    "sa_color='#4363d8'\n",
    "nn1w_color='#f58231'\n",
    "lrp_color='#ffe119'\n",
    "\n",
    "pa_sa  = plt.boxplot(pd.Series(standalone_kge_qle), notch=True, patch_artist=True, \n",
    "                     positions=[0], widths=[0.7], medianprops={'color': 'black'})\n",
    "pa_1w  = plt.boxplot(pd.Series(nn1w_kge_qle),       notch=True, patch_artist=True, \n",
    "                     positions=[1], widths=[0.7], medianprops={'color': 'black'})\n",
    "pa_lrp = plt.boxplot(pd.Series(lrp_kge_qle),        notch=True, patch_artist=True, \n",
    "                     positions=[2], widths=[0.7], medianprops={'color': 'black'})\n",
    "for patch in pa_sa['boxes']:\n",
    "    patch.set_facecolor(sa_color)\n",
    "for patch in pa_1w['boxes']:\n",
    "    patch.set_facecolor(nn1w_color)\n",
    "for patch in pa_lrp['boxes']:\n",
    "    patch.set_facecolor(lrp_color)\n",
    "\n",
    "\n",
    "pa_sa  = plt.boxplot(pd.Series(standalone_kge_qh),  notch=True, patch_artist=True, \n",
    "                     positions=[3.75], widths=[0.7], medianprops={'color': 'black'})\n",
    "pa_1w  = plt.boxplot(pd.Series(nn1w_kge_qh),        notch=True, patch_artist=True, \n",
    "                     positions=[4.75], widths=[0.7], medianprops={'color': 'black'})\n",
    "pa_lrp = plt.boxplot(pd.Series(lrp_kge_qh),         notch=True, patch_artist=True, \n",
    "                     positions=[5.75], widths=[0.7], medianprops={'color': 'black'})\n",
    "for patch in pa_sa['boxes']:\n",
    "    patch.set_facecolor(sa_color)\n",
    "for patch in pa_1w['boxes']:\n",
    "    patch.set_facecolor(nn1w_color)\n",
    "for patch in pa_lrp['boxes']:\n",
    "    patch.set_facecolor(lrp_color)\n",
    "\n",
    "plt.gca().set_ylim([-0.5, 1])\n",
    "plt.xticks([0, 1, 2, 3.75, 4.75, 5.75], labels=2*['SA', 'NN2W', 'NNLRP'])\n",
    "plt.text(0.3, -0.8, 'Latent Heat', fontsize=22)\n",
    "plt.text(4., -0.8, 'Sensible Heat', fontsize=22)\n",
    "plt.ylabel('KGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, ds in tqdm(site_ds.items()):\n",
    "    ds.to_netcdf(f'../data_for_paper_2/{s}_data.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_r_qle = {}\n",
    "site_r_qh = {}\n",
    "\n",
    "for site in tqdm(sim_sites):\n",
    "    new_input, new_output = etl_single_site(site_ds[site], use_mask=False)\n",
    "    #new_input = post_process_site(new_input)\n",
    "    t = site_ds[site]['time'].values\n",
    "    \n",
    "    analyzer = innvestigate.analyzer.LRPEpsilon(model, epsilon=1e-3, neuron_selection_mode='index')\n",
    "    r_qle_ds, r_qh_ds = analyze_model(new_input, t, analyzer)\n",
    "    site_r_qle[site] = r_qle_ds\n",
    "    site_r_qh[site] = r_qh_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, ds in tqdm(site_r_qle.items()):\n",
    "    ds.to_netcdf(f'../data_for_paper_2/{s}_lrp_qle.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, ds in tqdm(site_r_qh.items()):\n",
    "    ds.to_netcdf(f'../data_for_paper_2/{s}_lrp_qh.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
