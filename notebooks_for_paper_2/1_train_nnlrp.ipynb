{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NN to generate total latent heat given only forcing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "from IPython.display import SVG\n",
    "from tqdm.keras import TqdmCallback\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import plot_model, model_to_dot\n",
    "from tensorflow.keras import layers\n",
    "import dask.dataframe as dd\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "from KerasWeightsProcessing.convert_weights import txt_to_h5, h5_to_txt\n",
    "import pysumma.plotting as psp\n",
    "import pysumma.utils as psu\n",
    "import sklearn\n",
    "\n",
    "sns.set_context('talk')\n",
    "mpl.style.use('seaborn-bright')\n",
    "mpl.rcParams['figure.figsize'] = (18, 12)\n",
    "\n",
    "#strategy = tf.distribute.MirroredStrategy()\n",
    "#print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "dtype='float32'\n",
    "K.set_floatx(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_sites = os.listdir('../sites')\n",
    "\n",
    "seed = 50334\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(sim_sites)\n",
    "len(sim_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_dict = {s: xr.open_dataset(f'../sites/{s}/forcings/{s}.nc').isel(hru=0, drop=True).load() \n",
    "             for s in sim_sites}\n",
    "\n",
    "site_attr = {s: xr.open_dataset(f'../sites/{s}/params/local_attributes.nc').isel(hru=0, drop=True).load() \n",
    "             for s in sim_sites}\n",
    "\n",
    "site_outp = {s: xr.open_dataset(f'../sites/{s}/lrp_nn_output_{s}_timestep.nc').isel(hru=0, drop=True).load() \n",
    "             for s in sim_sites}\n",
    "\n",
    "site_parm = {s: xr.open_dataset(f'../sites/{s}/params/parameter_trial.nc').isel(hru=0, drop=True).load() \n",
    "             for s in sim_sites}\n",
    "\n",
    "def trim_time(sim, obs, roundto='min'):\n",
    "    sim['time'] = sim['time'].dt.round(roundto)\n",
    "    obs['time'] = obs['time'].dt.round(roundto)\n",
    "    sim_start = sim['time'].values[1]\n",
    "    sim_stop = sim['time'].values[-2]\n",
    "    obs_start = obs['time'].values[1]\n",
    "    obs_stop = obs['time'].values[-2]\n",
    "    start = max(sim_start, obs_start)\n",
    "    stop = min(sim_stop, obs_stop)\n",
    "    return slice(start, stop)\n",
    "\n",
    "\n",
    "for s in sim_sites:\n",
    "    # trim times to match\n",
    "    in_ds = site_dict[s]\n",
    "    outp_ds = site_outp[s]\n",
    "    ts = trim_time(in_ds, outp_ds)\n",
    "    in_ds = in_ds.sel(time=ts)\n",
    "    outp_ds = outp_ds.sel(time=ts)\n",
    "    site_dict[s] = in_ds\n",
    "    site_outp[s] = outp_ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_relhum(T, p, sh):\n",
    "    T0 = 273.16\n",
    "    return sh * (0.263*p)  / np.exp((17.67*(T-T0))/(T-29.65))\n",
    "\n",
    "def etl_single_site(in_ds, attr_ds, outp_ds, parm_ds, use_mask=True):\n",
    "   \n",
    "    #---------------------------------------------------------------------------\n",
    "    # Forcings\n",
    "    #---------------------------------------------------------------------------\n",
    "    airtemp   = (((in_ds['airtemp'].values / 27.315) - 10) / 2) + 0.5\n",
    "    spechum   = (in_ds['spechum'].values * 50)  \n",
    "    swradatm  = (in_ds['SWRadAtm'].values / 1000) \n",
    "    lwradatm  = in_ds['LWRadAtm'].values / (2 * 273.16)\n",
    "    pptrate   = np.around(10 * np.cbrt(in_ds['pptrate'].values), 2)\n",
    "    airpres   = (10 - (in_ds['airpres'].values / 10132.5)) / 3\n",
    "    windspd   = in_ds['windspd'].values / 10\n",
    "    mask      = in_ds['gap_filled'].values\n",
    "    relhumid  = calc_relhum(in_ds['airtemp'].values, in_ds['airpres'].values, in_ds['spechum']) / 100\n",
    "    relhumid[relhumid<0] = 0\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Parameters\n",
    "    #---------------------------------------------------------------------------\n",
    "    soiltype = attr_ds['soilTypeIndex'].values[()] * np.ones_like(mask) / 12\n",
    "    vegtype = attr_ds['vegTypeIndex'].values[()] * np.ones_like(mask) / 12\n",
    "    lai = outp_ds['scalarLAI'].values / 12\n",
    "    try:\n",
    "        canwidth = (parm_ds['heightCanopyTop']).values / 20\n",
    "    except:\n",
    "        canwidth = 1/(lai + 0.001)\n",
    "    \n",
    "    thetasat = parm_ds['theta_sat'].values[()] * np.ones_like(mask)\n",
    "    thetares = parm_ds['theta_res'].values[()] * np.ones_like(mask)\n",
    "    fieldcapacity = parm_ds['fieldCapacity'].values[()] * np.ones_like(mask)\n",
    "    soilwilting = parm_ds['critSoilWilting'].values[()] * np.ones_like(mask)\n",
    "    sm_min = soilwilting\n",
    "      \n",
    "    #---------------------------------------------------------------------------\n",
    "    # Soil moistures\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Surface moisture\n",
    "    nlayers_top = 0\n",
    "    nlayers_bot = 4\n",
    "    surf_idx = -len(outp_ds.midSoil)\n",
    "    surf_sm = outp_ds['mLayerVolFracWat'].copy(deep=True)\n",
    "    vmask = surf_sm != -9999\n",
    "    \n",
    "    depth = outp_ds['mLayerHeight'].copy(deep=True)\n",
    "    dmask = depth != -9999\n",
    "    depth.values = psp.utils.justify(depth.where(dmask).values)\n",
    "    depth = depth.isel(midToto=slice(surf_idx+nlayers_top, surf_idx+nlayers_bot))\n",
    "    depth = depth / depth.sum(dim='midToto')\n",
    "    \n",
    "    surf_sm.values = psp.utils.justify(surf_sm.where(vmask).values)\n",
    "    surf_sm = surf_sm.isel(midToto=slice(surf_idx+nlayers_top, surf_idx+nlayers_bot))\n",
    "    surf_sm *= depth\n",
    "    surf_sm = surf_sm.sum(dim='midToto')\n",
    "    surf_sm = (surf_sm - sm_min[0]) / (thetasat[0] - sm_min[0])\n",
    "   \n",
    "    #---------------------------------------------------------------------------\n",
    "    # Arrange inputs and outputs\n",
    "    #---------------------------------------------------------------------------    \n",
    "    train_input = np.vstack([airtemp,       # 0\n",
    "                             relhumid,      # 1\n",
    "                             swradatm,      # 2\n",
    "                             surf_sm,       # 3\n",
    "                             lai * canwidth, # 4\n",
    "                             vegtype,       # 5\n",
    "                            ]).T \n",
    "    train_output = np.vstack([in_ds['Qle_cor'].values / 500,\n",
    "                              in_ds['Qh_cor'].values / 500,]).T\n",
    "    \n",
    "    if use_mask:\n",
    "        train_input = train_input[mask == 0]\n",
    "        train_output = train_output[mask == 0]    \n",
    "    return train_input.astype(np.float32), train_output.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# load in data, transform, and split for training\n",
    "# -----------------------------------------------\n",
    "train_data = [etl_single_site(site_dict[s], site_attr[s], site_outp[s], site_parm[s]) for s in sim_sites]\n",
    "\n",
    "train_input = np.vstack([td[0] for td in train_data])\n",
    "train_output = np.vstack([td[1] for td in train_data])\n",
    "\n",
    "index_shuffle = np.arange(train_output.shape[0])\n",
    "np.random.shuffle(index_shuffle)\n",
    "train_input = train_input[index_shuffle, :]\n",
    "train_output = train_output[index_shuffle, :]\n",
    "\n",
    "validation_frac = 0.2\n",
    "validation_start_idx = int(train_output.shape[0] * (1-validation_frac))\n",
    "\n",
    "train_input, valid_input = train_input[0:validation_start_idx, :], train_input[validation_start_idx:, :]\n",
    "train_output, valid_output = train_output[0:validation_start_idx], train_output[validation_start_idx:]    \n",
    "\n",
    "assert np.isnan(train_input).sum() + np.isnan(train_output).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Define model hyperparameters\n",
    "# -----------------------------------------------\n",
    "model_name = f'simple_dense_dropout'\n",
    "loss = 'mse'\n",
    "activation = 'tanh'    \n",
    "width = 28\n",
    "dropout_rate = 0.1\n",
    "epochs = 50\n",
    "batch_size = 48 * 28\n",
    "num_layers = 2\n",
    "use_dropout = True\n",
    "optimizer = 'adam' \n",
    "\n",
    "# -----------------------------------------------\n",
    "# Define model structure\n",
    "# -----------------------------------------------\n",
    "input_layer = layers.Input(shape=train_input[0].shape)\n",
    "l = input_layer\n",
    "\n",
    "for _ in range(num_layers):\n",
    "    l = layers.Dense(width, activation=activation)(l)\n",
    "    if use_dropout:\n",
    "        l = layers.Dropout(dropout_rate)(l)\n",
    "output_layer = layers.Dense(train_output.shape[-1], activation='linear')(l)\n",
    "\n",
    "model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Train the model\n",
    "# -----------------------------------------------\n",
    "history = model.fit(\n",
    "    train_input, train_output,\n",
    "    validation_data=(valid_input, valid_output),\n",
    "    batch_size=batch_size, epochs=epochs, shuffle=True, verbose=1, \n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Save the model and history\n",
    "# -----------------------------------------------\n",
    "# save the history     \n",
    "hist_df = pd.DataFrame(history.history) \n",
    "hist_csv_file = f'../new_models/{model_name}.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)\n",
    "\n",
    "# save the model    \n",
    "model.save(f'../new_models/{model_name}.h5')\n",
    "h5_to_txt(\n",
    "    weights_file_name=f'../new_models/{model_name}.h5', \n",
    "    output_file_name=f'../new_models/{model_name}.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (18, 12) \n",
    "plt.plot(history.history['loss'][:], label='training', linewidth=3, color='tomato')\n",
    "plt.plot(history.history['val_loss'][:], label='validation', linewidth=3, color='olivedrab')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
